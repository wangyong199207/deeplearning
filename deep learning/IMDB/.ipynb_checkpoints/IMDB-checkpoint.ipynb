{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw = open(\"imdb.pkl\", 'rb')\n",
    "Raw = pickle.load(raw)\n",
    "raw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dataset_file(dataset, default_dataset, origin):\n",
    "    '''Look for it as if it was a full path, if not, try local file,\n",
    "    if not try in the data directory.\n",
    "\n",
    "    Download dataset if it is not present\n",
    "\n",
    "    '''\n",
    "    data_dir, data_file = os.path.split(dataset)\n",
    "    if data_dir == \"\" and not os.path.isfile(dataset):\n",
    "        # Check if dataset is in the data directory.\n",
    "        new_path = os.path.join(\n",
    "            os.path.split(__file__)[0],\n",
    "            \"..\",\n",
    "            \"data\",\n",
    "            dataset\n",
    "        )\n",
    "        if os.path.isfile(new_path) or data_file == default_dataset:\n",
    "            dataset = new_path\n",
    "\n",
    "    if (not os.path.isfile(dataset)) and data_file == default_dataset:\n",
    "        from six.moves import urllib\n",
    "        print('Downloading data from %s' % origin)\n",
    "        urllib.request.urlretrieve(origin, dataset)\n",
    "\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open(\"imdb.pkl\", 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "train_set = pickle.load(f)\n",
    "test_set = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "if maxlen:\n",
    "    new_train_set_x = []\n",
    "    new_train_set_y = []\n",
    "    for x, y in zip(train_set[0], train_set[1]):\n",
    "        if len(x) < maxlen:\n",
    "            new_train_set_x.append(x)\n",
    "            new_train_set_y.append(y)\n",
    "    train_set = (new_train_set_x, new_train_set_y)\n",
    "    del new_train_set_x, new_train_set_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_x, train_set_y = train_set\n",
    "n_samples = len(train_set_x)\n",
    "sidx = numpy.random.permutation(n_samples)\n",
    "n_train = int(numpy.round(n_samples * (1. - valid_portion)))\n",
    "valid_set_x = [train_set_x[s] for s in sidx[n_train:]]\n",
    "valid_set_y = [train_set_y[s] for s in sidx[n_train:]]\n",
    "train_set_x = [train_set_x[s] for s in sidx[:n_train]]\n",
    "train_set_y = [train_set_y[s] for s in sidx[:n_train]]\n",
    "\n",
    "train_set = (train_set_x, train_set_y)\n",
    "valid_set = (valid_set_x, valid_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(path=\"imdb.pkl\", n_words=100000, valid_portion=0.1, maxlen=None,\n",
    "              sort_by_len=True):\n",
    "    '''Loads the dataset\n",
    "\n",
    "    :type path: String\n",
    "    :param path: The path to the dataset (here IMDB)\n",
    "    :type n_words: int\n",
    "    :param n_words: The number of word to keep in the vocabulary.\n",
    "        All extra words are set to unknow (1).\n",
    "    :type valid_portion: float\n",
    "    :param valid_portion: The proportion of the full train set used for\n",
    "        the validation set.\n",
    "    :type maxlen: None or positive int\n",
    "    :param maxlen: the max sequence length we use in the train/valid set.\n",
    "    :type sort_by_len: bool\n",
    "    :name sort_by_len: Sort by the sequence lenght for the train,\n",
    "        valid and test set. This allow faster execution as it cause\n",
    "        less padding per minibatch. Another mechanism must be used to\n",
    "        shuffle the train set at each epoch.\n",
    "\n",
    "    '''\n",
    "\n",
    "    #############\n",
    "    # LOAD DATA #\n",
    "    #############\n",
    "\n",
    "    # Load the dataset\n",
    "    path = get_dataset_file(\n",
    "        path, \"imdb.pkl\",\n",
    "        \"http://www.iro.umontreal.ca/~lisa/deep/data/imdb.pkl\")\n",
    "\n",
    "    if path.endswith(\".gz\"):\n",
    "        f = gzip.open(path, 'rb')\n",
    "    else:\n",
    "        f = open(path, 'rb')\n",
    "\n",
    "    train_set = pickle.load(f)\n",
    "    test_set = pickle.load(f)\n",
    "    f.close()\n",
    "    if maxlen:\n",
    "        new_train_set_x = []\n",
    "        new_train_set_y = []\n",
    "        for x, y in zip(train_set[0], train_set[1]):\n",
    "            if len(x) < maxlen:\n",
    "                new_train_set_x.append(x)\n",
    "                new_train_set_y.append(y)\n",
    "        train_set = (new_train_set_x, new_train_set_y)\n",
    "        del new_train_set_x, new_train_set_y\n",
    "\n",
    "    # split training set into validation set\n",
    "    train_set_x, train_set_y = train_set\n",
    "    n_samples = len(train_set_x)\n",
    "    sidx = numpy.random.permutation(n_samples)\n",
    "    n_train = int(numpy.round(n_samples * (1. - valid_portion)))\n",
    "    valid_set_x = [train_set_x[s] for s in sidx[n_train:]]\n",
    "    valid_set_y = [train_set_y[s] for s in sidx[n_train:]]\n",
    "    train_set_x = [train_set_x[s] for s in sidx[:n_train]]\n",
    "    train_set_y = [train_set_y[s] for s in sidx[:n_train]]\n",
    "\n",
    "    train_set = (train_set_x, train_set_y)\n",
    "    valid_set = (valid_set_x, valid_set_y)\n",
    "\n",
    "    def remove_unk(x):\n",
    "        return [[1 if w >= n_words else w for w in sen] for sen in x]\n",
    "\n",
    "    test_set_x, test_set_y = test_set\n",
    "    valid_set_x, valid_set_y = valid_set\n",
    "    train_set_x, train_set_y = train_set\n",
    "\n",
    "    train_set_x = remove_unk(train_set_x)\n",
    "    valid_set_x = remove_unk(valid_set_x)\n",
    "    test_set_x = remove_unk(test_set_x)\n",
    "\n",
    "    def len_argsort(seq):\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "\n",
    "    if sort_by_len:\n",
    "        sorted_index = len_argsort(test_set_x)\n",
    "        test_set_x = [test_set_x[i] for i in sorted_index]\n",
    "        test_set_y = [test_set_y[i] for i in sorted_index]\n",
    "\n",
    "        sorted_index = len_argsort(valid_set_x)\n",
    "        valid_set_x = [valid_set_x[i] for i in sorted_index]\n",
    "        valid_set_y = [valid_set_y[i] for i in sorted_index]\n",
    "\n",
    "        sorted_index = len_argsort(train_set_x)\n",
    "        train_set_x = [train_set_x[i] for i in sorted_index]\n",
    "        train_set_y = [train_set_y[i] for i in sorted_index]\n",
    "\n",
    "    train = (train_set_x, train_set_y)\n",
    "    valid = (valid_set_x, valid_set_y)\n",
    "    test = (test_set_x, test_set_y)\n",
    "\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
