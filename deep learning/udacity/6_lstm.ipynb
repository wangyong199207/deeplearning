{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "    \n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits, tf.concat(0, train_labels)))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.300584 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.13\n",
      "================================================================================\n",
      "kq d e n ol i xeztjgl e myypw lsuldg hfrjoofr keemelohytksneeztgeb nvsovi na toe\n",
      "rsdhntunib h vaqvahueh e ziesjeh  g reb k iixetnethy ktlxzacpeikreokowoluue bbet\n",
      "wysl jz ys tgewpsze darxxleinsjxravmhi uwzernlhtm uysavrajnszrwwagmrer y ux mnqt\n",
      "nor ejodpudikyu ry ru  laqbyty ayoprdlz ehsnebtel  eailyfj  moechasis dalosvtslo\n",
      "jl dcon fio ertgsatrpemju  a  anyqfikjguwt hns  sion e sgoidxryykmomqkbozluqliss\n",
      "n of the epown only foanitarian r mellon uni down one nhematician ed part of  dispersal imary missiresidential four perceuage standaomotives rtt fruit neciablo when des paraphyy can get ition of musach of themkes them deost understernal in itthe older gns of philomeo roles mr doom and ibed above rman troopsnnihilationive realityprisoning m sighet bec first ladyhe one nineuter storag symbolic iions of ghaon a properes of war aburning of  workers thred to as tember two znt altes rament adviso then the ber of irisheties pictuzero zero tr five one rayed imporar ruler beow in pokerry localisethen presenvice versa t cell lateh the same ainstream cst group evnelia s jewsheries sciy known as d were fire\n",
      "================================================================================\n",
      "Validation set perplexity: 20.06\n",
      "Average loss at step 100: 2.590143 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.11\n",
      "Validation set perplexity: 10.52\n",
      "Average loss at step 200: 2.267472 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.57\n",
      "Validation set perplexity: 9.13\n",
      "Average loss at step 300: 2.110253 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.79\n",
      "Validation set perplexity: 8.16\n",
      "Average loss at step 400: 2.024960 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.98\n",
      "Validation set perplexity: 7.66\n",
      "Average loss at step 500: 1.955260 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.67\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 600: 1.927150 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.02\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 700: 1.887426 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.48\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 800: 1.849268 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 900: 1.815691 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 1000: 1.843135 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.55\n",
      "================================================================================\n",
      "tologd englm clacker depa mapport of zeromer caffluace and pourty intownared abo\n",
      "bertoogh ty and six six as toaganidy chiles atrotent witest in the play bojopht \n",
      "zingss puriralyson jrobmx folm their of andnows of not to mortobly the alauplem \n",
      "pives offant of drick and actool arerial pout extracters and hawsioned alistern \n",
      "y dojlica the tame daytritated has noje pregentalla a front zero sive seven ss s\n",
      "to a baptisrtion is so it unfit f the angle ra byzantinswick andrer and preseo alkanes nbra akira mginal dynasnconcerned story one sentral scotthe wildlifmote checkr the day ofth of reporool room dical spaces ny romano c eight seveversion arto be abruptad positione appeared  zero one tas initialladen wernerd edition aselfless loere ended bk schocken  the sardaunation list of extendeto avoid rethern savanrameters so over satele four six ot just the service tothe oslo acboy pocket nd monitorsf church trorton desmoarties with of which adiers surrea in one niibes practiaudiences tts of what ix one mille arte antid predominae of slaver one nine s another cotralia and  it also mated higher ve million \n",
      "================================================================================\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 1100: 1.786864 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 1200: 1.795771 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 6.11\n",
      "Average loss at step 1300: 1.764721 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.61\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 1400: 1.772023 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 5.98\n",
      "Average loss at step 1500: 1.769466 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 1600: 1.704432 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 5.78\n",
      "Average loss at step 1700: 1.693361 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 1800: 1.711030 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 1900: 1.734476 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 2000: 1.694908 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "================================================================================\n",
      "weder shoitute vile an were was doven to a the troilctle cogns four pho lave ina\n",
      "qud whichiemed the seven the sestest indosides four which intervied who process \n",
      "juiblish is premoriea phantre ever nannor leesoqy one five nast strib to recoint\n",
      "que iscuess yluck to or glosteth of artial spupos with on was matti is a restrou\n",
      "nt in and two to g stader usual ox europe from seveen ffoe mothed us it an screv\n",
      "rm the new mber of reclcohol anywt the crownant p one er augustine caapi was ix is addeding the sto domestic ppeking univally botanyves and shuno evidence stadium auarting fromzombies as n antidote te set admied by some ece and puttead some pive eight out to be on at universstreets on t nine in od processin stoiber int age the fmajesty as attitude ant two the lply includethree it waplaced withp narrow goe manipulattisfiable thanges in w productivee people aten ordered rt or salesnited kingdll simply nee the qur fulfill thereceives itala mara fips an obses also knownsed her sectom from netants was nhe portugue for the trng plants whe pre one ed in his tsemi hypnotmong monohu included aory element\n",
      "================================================================================\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2100: 1.695001 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2200: 1.710691 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 2300: 1.697856 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2400: 1.640089 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2500: 1.659229 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2600: 1.673597 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2700: 1.658920 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2800: 1.643610 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2900: 1.640720 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 3000: 1.645194 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "================================================================================\n",
      "s are to the be intellinish efgorntion the chian brash bteab daxan inkineth amer\n",
      "rap two zero zero five enbarking allunst a that then being oull appedinly rigent\n",
      "f at internations and not dieman jearting discraing and from wro wrands energs c\n",
      "p of rinction baquenal one sixch rigan general theon trantle an karain that to g\n",
      "on quay renicers and was are degated the sent docultian conviled vings ho kinena\n",
      "reafter incl of canading by the c in sandy men of as amthe donatisro three ayven eight fion intelliation icao ear was the zeiger e ot if job woe th centurounded by b the inflatth are derich is molesr demille i one eight agan hypoth forms has er on the rrformance ad on the boften the rety of creatreceived in and loxianor an annulto be a lones of activix a new fot will do aell universs service ao five mi tyear since ng a g lafl different y has been vailable onsearch deven the jahrb five citie settlementctives on ty the genev zero five  nobel prizjakarta cith national wo one may  were replaur and one of portugalar songhai  state reptorically lit seven the target andirtual dos such as theael smith j\n",
      "================================================================================\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 3100: 1.672899 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3200: 1.644052 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 3300: 1.642402 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3400: 1.664315 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3500: 1.641124 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3600: 1.620210 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 3700: 1.624102 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 3800: 1.647432 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 3900: 1.653773 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 4000: 1.638343 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "================================================================================\n",
      "and limited the cintol native compart of the sons which that ggoupriogieg that t\n",
      "ing jek taghip ore foures of the peotogen chaction enverfout of dideam of the a \n",
      "y east enperson bogh and were stotate armageme that numping have be ratimemes to\n",
      "perimist willied jolequlamics bogas unwold are results with counterly faito joni\n",
      "cs mixmarps of protorioned of to portwarlering life signacil commonnecrocaty thr\n",
      "e of mobilen abstract liam kemmlemenia made n later bool universitagonny riseficiency thh gould havituted abou share a lehe environmr darkness n bizkaia dews report supervise tranc the repproximatioe for immedgetting off appraisalsld be typictory shows  in the falor divinatiigital signwere to be ither aristita symbol  computer png with caner non tremlowed teamsto pages threditary liletters lik civilian gversely ozowant to tely cheer go t very nighact that thnathan linerinting metrles wood mif too manyt cloud rob on hudson ine five eied one ninelso jakartaomy main artain allowiapan the hof liberateder name forings of gea but is eit of the mal helene falass to protfic accompled to have led by the \n",
      "================================================================================\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 4100: 1.639169 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 4200: 1.615145 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 4300: 1.615746 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4400: 1.633868 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 4500: 1.613826 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4600: 1.614957 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 4700: 1.612072 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 4800: 1.607369 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 4900: 1.592228 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 5000: 1.610190 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "================================================================================\n",
      "ult languages free abotand of phosoning this t s events and banaany and wife see\n",
      "um irian avet which implafrially dos sequences that foundies lint the vebloss sm\n",
      "y dune manuli rook linable transevical hid thought langen particus one prhmpatua\n",
      "y othem monthyance s that he percent guespotical swan sit that the londughag dan\n",
      "vies was a one zero onem united lardino for the colls woll dolp statigism poluct\n",
      "omized by ot of the gat pope paul of a word sraelites ao zero zerocity of sous up to fouca let s bes of the raen dave shas are the bth the writge both modo took eyewificaly untc c f a thees countingnerally acche romans oproviding te five zerosubscriber ity of theicle discussthe grid onus proposedeages develis the appa well as a on jonathants new yorkn since theated to wiklation of tet reached twave threeext millennert tour wiending on witu t serieiguring a megion hezbopose and efland and sen the umm aor out of pich complexen six one one nine onhe world evducation isy actress jography kolgovernment  holds occavernors gen michigan splans for che long run game commo a virgin mranger nineich are des\n",
      "================================================================================\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 5100: 1.587741 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5200: 1.580755 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5300: 1.577708 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5400: 1.586986 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.04\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5500: 1.593277 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5600: 1.565688 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5700: 1.560901 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5800: 1.603008 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5900: 1.598742 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6000: 1.589947 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "================================================================================\n",
      "vers and other idman and eight willia in interradian two of the emerate a gated \n",
      "ques geographous the caypomder and in one nine sidezen two three of the other in\n",
      "plaiment on the mavors was ot golderen tax rile was creaking regaris yeay peosac\n",
      "um actorrilions in the aulogance gider was caulled a raptowiblan appearing pont \n",
      "ef idman boy on can produce pares empayy antilrakeltred by and german lincoln an\n",
      "bs tv quiz  ball a fum the next ond ammonium eight zeroers auxilia to four yeh installer the originnize the onhe sugar ththe area being areas te idea thato bo diddlehe human eyme ground and an obsersound barri and b of rde of the cthe early htheir numbeal origins trictive arof flax dis north bankindividualsn generallymy cradle ame one of t to changinself bettereven two zerticularly our eight zred the ter to prevent to have thtestants coe one reput which latenued fightirred in afrultiplicatira daughtern two bordounce still ader medgarodel and acion geller es cooled re influencears in priso smaller p in an insunavarre spaa minimum wessions eac jacob talmn bus this thasha oldfamna the car all cards\n",
      "================================================================================\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6100: 1.573276 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6200: 1.558447 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6300: 1.576619 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6400: 1.606044 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6500: 1.627048 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6600: 1.616915 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6700: 1.589489 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6800: 1.577819 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6900: 1.577489 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 7000: 1.608057 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "================================================================================\n",
      "ce of ramme polachigfteence thour t reached manwern polayine releaced and once t\n",
      "radia discon the exciacies doters velies haven europe c cavreties of discovering\n",
      "fional rar madesia has charge sepaature on higs tameta can pertopances chare lon\n",
      "lated also is nide dging in one nine six five zero zero five seven five six eigh\n",
      "bolebell calque at was one cires about the eayy the son that schoodd magy karatu\n",
      "t of a comp who handlee paper seee catalyst diet therefrrival of t the binarys in one ni opinions h deny that then be linfree electrero zero zeapplets of  listed as he one cardeffable intor groups bmp and crabent had disain memory ne nine ninal cable onn linear dnne nine ninse its wate itself founcestor to tended destce fluorescetal waveguquis yi of ormations te nose albeation of huefinition sdical treatof gumby infor female e nine nine a slowly ch east at tne literatu the ice agd curve fits killed acmmunotherapplaying chaecond bank choreographne nine sixnon kazakhsial arts cathe southeren male one the town oent electedvocate linkhigh and chstem in indtrike a goox is the numarquette bribor lies \n",
      "================================================================================\n",
      "Validation set perplexity: 4.53\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print(''.join(batches2string(batches)))\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
